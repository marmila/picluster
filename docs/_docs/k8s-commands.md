---
title: Kubernetes commands
permalink: /docs/k8s-commands/
description: Reference of kubectl/helm commands for our Kuberentes Raspberry Pi Cluster
last_modified_at: "03-04-2022"
---

## Kubectl Commands

- List PODs running on an specific node

  ```shell
  kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=<node_name>
  ```

- List Taints of all nodes

  ```shell
  kubectl describe nodes | grep Taint
  ```

- Restart pod

  ```shell
  kubectl rollout restart daemonset/deployment/statefulset <daemonset/deployment/statefulset>
  ```

- Get logs from a pod

  ```shell
  kubectl logs <pod_name> <container_name> -n <namespace> 
  ```

- Connect to a container

  ```shell
  kubectl exec -it <pod_name> -c <container_name> -n <namespace> -- /bin/bash
  ```

- Service port forwarding

  ```shell
  kubectl port-forward svc/[service-name] -n [namespace] [external-port]:[internal-port] --addess 0.0.0.0
  ```

  Port forwarding from binding service `service-name` listening on `internal_port` to `0.0.0.0:external_port`


- Getting nodes memory/cpu usage

  ```shell
  kubectl top nodes
  ```

- Getting top pods sort by cpu

  ```shell
  kubectl top pods -A --sort-by='cpu'
  ```

- Getting top pods sort by memory

  ```shell
  kubectl top pods -A --sort-by='memory'
  ```

  {{site.data.alerts.note}}

  `kubectl top` uses kubernetes metrics API. [metrics server](https://github.com/kubernetes-sigs/metrics-server)need to be installed, which by default is installed in K3S

  {{site.data.alerts.end}}

## How to run curl in Kubernetes (for troubleshooting)

Run curl commands against any POD or service endpoint, running a new pod containing using a utility image containing `curl` command.

Example use [official `curl` docker image](https://hub.docker.com/r/curlimages/curl) which support multiarch (amd64 and arm64)

```
kubectl run -it --rm --image=curlimages/curl curly -- sh
```

{{site.data.alerts.note}}

`busybox` image is another useful POD for troubleshooting, but it does not contain `curl` commad (it contains `wget`)

{{site.data.alerts.end}}


## Patching Helm manifest files on the fly using Kustomize

Helm provides the possibility of manipulate, configure, and/or validate rendered manifests before they are installed by Helm: [`--post-rendering` option](https://helm.sh/docs/topics/advanced/#post-rendering). This enables the use of [`kustomize`](https://kustomize.io/) to apply configuration changes without the need to fork a public chart or requiring chart maintainers to specify every last configuration option for a piece of software.

Since v1.14, `kubectl` includes [`kustomize` support](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/):

```shell
kubectl kustomize <kustomization_directory>
kubectl apply -k <kustomization_directory>
```

Based on procedure described [in this post](https://alysivji.github.io/helm-post-rendering-hook.html) kustomize can be used to apply patches to manifest files generated by Helm before install them.



- Step 1: Create directory `kustomize`

  ```
  mkdir kustomize
  ```

- Step 2: Create `kustomize` wrapper script within `kustomize` directory
  
  ```shell
  #!/bin/bash

  # save incoming YAML to file
  cat <&0 > all.yaml

  # modify the YAML with kustomize
  kubectl kustomize . && rm all.yaml
  ``` 
  
  The script simply save all incomming manifest files from helm chart to a temporal file `all.yaml` and then execute `kubectl kustomize` to the current directory, applying kustomize transformations, and finally remove the temporal file

- Step 3: Create kutomize files. In this example, a environment variable (`POD_IP`) within DaemonSet `longhorn-manager` will be patched with a new value.

  `kustomization.yml`
  ```yml
  apiVersion: kustomize.config.k8s.io/v1beta1
  kind: Kustomization

  resources:
    - all.yaml
  patches:
    - path: patch.yaml
      target:
        kind: DaemonSet
        name: "longhorn-manager"
  ```

  This file indicates to patch DaemonSet `longhorn-manager` within `all.yml` file using `patch.yaml`

  `patch.yaml`
  ```yml
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    name: longhorn-manager
  spec:
    template:
      spec:
        containers:
          - name: longhorn-manager # (1)
            env:
              - name: POD_IP
                value: 0.0.0.0
                valueFrom:
  ```
  NOTE: It is needed to set null to key `valueFrom` in order to delete previous value.
  

  - Step 3: Execute dry-run of helm install to see the changes in the manifests files

    ```shell
    helm install longhorn longhorn/longhorn -f ../longhorn_values.yml --post-renderer ./kustomize --debug --dry-run
    ```

  - Step 4: Deploy the helm
    
    ```shell
    helm install longhorn longhorn/longhorn -f ../longhorn_values.yml --post-renderer ./kustomize --namespace longhorn-system
    ```

  {{site.data.alerts.note}}
  
  Ansible does not support yet --post-rendering option to helm module. There is [open issue in kubernetes core asible collection](https://github.com/ansible-collections/kubernetes.core/issues/30) for providing this functionallity.

  {{site.data.alerts.end}}


## Move pods from one node to another

In case one pods need to be executed in other node, maybe because it is pushig the node to its limits in terms of resources and there is another node less used.

The procedure is the following:

- Step 1: Get information about the node where the pod is running
  
  ```shell
  kubectl get pod <pod-name> -n <namespace> -o wide
  ```

- Step 2: Cordon the node where the pod is currently running, so Kubernetes scheduler cannot use it to schedule new PODs 

  ```shell
  kubectl cordon <node>
  ```
  
  {{site.data.alerts.note}}

  Kubernetes cordon is an operation that marks or taints a node in your existing node pool as unschedulable. By using it on a node, you can be sure that no new pods will be scheduled for this node. The command prevents the Kubernetes scheduler from placing new pods onto that node, but it doesnâ€™t affect existing pods on that node.
  
  {{site.data.alerts.end}}

- Step 3: Delete POD. It is assumed that POD is controlled by a replica set or statefulset so after deleting it, Kubernetes will reschedule it automatically in any node which is not cordoned

  ```shell
  kubectl delete pod <pod> -n <namespace>
  ```

- Step 4: Check the POD is started in another node

- Step 5: Uncordon the node, so it can be used again to schedule pods.
  
  ```shell
  kubectl uncordon <node>
  ```