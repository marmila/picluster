---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-main-config
  annotations:
    # Disable variable substitution
    kustomize.toolkit.fluxcd.io/substitute: disabled
data:
  fluent.conf: |-
    # Include conf files in config.d directory
    @include config.d/*.conf

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-extra-files
  annotations:
    # Disable variable substitution
    kustomize.toolkit.fluxcd.io/substitute: disabled
data:
  01_sources.conf: |-
    ## FluentD config file
    ## Sources
    ##
    ## logs from fluentbit forwarders
    <source>
        @type forward
        @label @FORWARD
        bind "#{ENV['FLUENTD_FORWARD_BIND'] || '0.0.0.0'}"
        port "#{ENV['FLUENTD_FORWARD_PORT'] || '24224'}"
        # Enabling TLS
        <transport tls>
            cert_path /etc/fluent/certs/tls.crt
            private_key_path /etc/fluent/certs/tls.key
        </transport>
        # Enabling access security
        <security>
        self_hostname "#{ENV['FLUENTD_FORWARD_SEC_SELFHOSTNAME'] || 'fluentd-aggregator'}"
        shared_key "#{ENV['FLUENT_AGGREGATOR_SHARED_KEY'] || 'sharedkey'}"
        </security>
    </source>
    ## Enable Prometheus end point
    <source>
        @type prometheus
        @id in_prometheus
        bind "0.0.0.0"
        port 24231
        metrics_path "/metrics"
    </source>
    <source>
        @type prometheus_monitor
        @id in_prometheus_monitor
    </source>
    <source>
        @type prometheus_output_monitor
        @id in_prometheus_output_monitor
    </source>
    ## Enable syslog endpoint
    <source>
        @type syslog
        @label @FORWARD
        tag host
        bind 0.0.0.0
        port 5140
        protocol_type udp
        <parse>
            message_format rfc3164
        </parse>
    </source>

  02_filters.conf: |-  
    ##
    ## Filters
    ##
    <label @FORWARD>
        # Re-route fluentd logs
        <match kube.var.log.containers.fluentd**>
            @type relabel
            @label @FLUENT_LOG
        </match>
        ## Get kubernetes fields
        # <filter kube.**>
        #     @type record_modifier
        #     remove_keys kubernetes, __dummy__, __dummy2__
        #     <record>
        #         __dummy__   ${ p = record["kubernetes"]["labels"]["app"]; p.nil? ? p : record['app'] = p; }
        #         __dummy2__   ${ p = record["kubernetes"]["labels"]["app.kubernetes.io/name"]; p.nil? ? p : record['app'] = p; }
        #         namespace ${ record.dig("kubernetes","namespace_name") }
        #         pod ${ record.dig("kubernetes", "pod_name") }
        #         container ${ record.dig("kubernetes", "container_name") }
        #         host ${ record.dig("kubernetes", "host")}
        #     </record>
        # </filter>
        <match **>
            @type relabel
            @label @DISPATCH
        </match>
    </label>
  03_dispatch.conf: |-
    # Discard FLUENTD LOGS
    <label @FLUENT_LOG>
      <match **>
        @type null
        @id ignore_fluent_logs
      </match>
    </label>
    # Dispatch logs to different destinations
    <label @DISPATCH>
        # Calculate prometheus metrics
        <filter **>
            @type prometheus
            <metric>
                name fluentd_input_status_num_records_total
                type counter
                desc The total number of incoming records
                <labels>
                tag ${tag}
                hostname ${host}
                </labels>
            </metric>
        </filter>
        # Copy log stream to different outputs
        <match **>
            @type copy
            <store>
                @type relabel
                @label @OUTPUT_ES
            </store>
            <store>
                @type relabel
                @label @OUTPUT_LOKI
            </store>  
        </match>
    </label>
  04_output.conf: |-
    #
    # Output
    #
    # Elastic Search Output
    <label @OUTPUT_ES>
        # Setup index name. Index per namespace or per container
        <filter kube.**>
            @type record_transformer
            enable_ruby
            <record>
                index_name ${record['namespace']}
            </record>
        </filter>
        <filter host.**>
            @type record_transformer
            enable_ruby
            <record>
                index_name "host"
            </record>
        </filter>
        # Send received logs to elasticsearch
        <match **>
            @type elasticsearch
            @id out_es
            @log_level info
            include_tag_key true
            host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
            port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
            scheme http
            user "#{ENV['FLUENT_ELASTICSEARCH_USER'] || use_default}"
            password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD'] || use_default}"
            # Reload and reconnect options
            reconnect_on_error true
            reload_on_failure true
            reload_connections false
            # HTTP request timeout
            request_timeout 15s
            # Log ES HTTP API errors
            log_es_400_reason true
            # avoid 7.x errors
            suppress_type_name true
            # setting sniffer class
            sniffer_class_name Fluent::Plugin::ElasticsearchSimpleSniffer
            # Do not use logstash format
            logstash_format false
            # Setting index_name
            index_name fluentd-${index_name}
            # specifying time key
            time_key time
            # including @timestamp field
            include_timestamp true
            # ILM Settings - WITH ROLLOVER support
            # https://github.com/uken/fluent-plugin-elasticsearch/blob/master/README.Troubleshooting.md#enable-index-lifecycle-management
            index_date_pattern ""
            enable_ilm true
            ilm_policy_id fluentd-policy
            ilm_policy {"policy":{"phases":{"hot":{"min_age":"0ms","actions":{"rollover":{"max_size":"10gb","max_age":"7d"}}},"warm":{"min_age":"2d","actions":{"shrink":{"number_of_shards":1},"forcemerge":{"max_num_segments":1}}},"delete":{"min_age":"7d","actions":{"delete":{"delete_searchable_snapshot":true}}}}}}
            ilm_policy_overwrite true
            # index template
            use_legacy_template false
            template_overwrite true
            template_name fluentd-${index_name}
            template_file "/etc/fluent/template/fluentd-es-template.json"
            customize_template {"<<shard>>": "1","<<replica>>": "0", "<<TAG>>":"${index_name}"}
            remove_keys index_name
            <buffer tag, index_name>
                flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
                flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
                chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
                queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
                retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
                retry_forever true
            </buffer>
        </match>
    </label>
    # Loki Output
    <label @OUTPUT_LOKI>
        <match **>
            @type loki
            @id out_loki_kube
            @log_level info
            url "#{ENV['LOKI_URL']}"
            username "#{ENV['LOKI_USERNAME'] || use_default}"
            password "#{ENV['LOKI_PASSWORDD'] || use_default}"
            extra_labels {"job": "fluentd"}
            line_format json
            <label>
                app
                container
                pod
                namespace
                host
                filename
            </label>
            <buffer>
                flush_thread_count 8
                flush_interval 5s
                chunk_limit_size 2M
                queue_limit_length 32
                retry_max_interval 30
                retry_forever true
            </buffer>
        </match>
    </label>